function agent = createTD3Agent(numObs, obsInfo, numAct, actInfo, Ts)
% Enhanced TD3 agent setup for quadruped robot locomotion
% Based on Lillicrap et al. (2015) and Fujimoto et al. (2018)
% Adds noise decay, improved initialization, and LayerNorm networks

%% === Create Neural Networks ===
[criticNetwork1, criticNetwork2, actorNetwork] = createNetworks(numObs, numAct, actInfo);
figure;
plot(criticNetwork1)
title("Krytyk 1")
figure;
plot(criticNetwork2)
title("Krytyk 2")
% Plot the actor network for visualization
figure;
plot(actorNetwork);
title("Aktor")


%% === Critic Optimizer Options ===
criticOpts = rlOptimizerOptions( ...
    Optimizer = "adam", ...
    LearnRate = 1e-3, ...                 % Slightly faster critic learning
    GradientThreshold = 1, ...
    L2RegularizationFactor = 2e-4);

%% === Actor Optimizer Options ===
actorOpts = rlOptimizerOptions( ...
    Optimizer = "adam", ...
    LearnRate = 1e-4, ...                 % Actor learns faster (typical TD3 ratio ~5:1)
    GradientThreshold = 1, ...
    L2RegularizationFactor = 1e-5);

%% === Define Critics ===
critic1 = rlQValueFunction(criticNetwork1, obsInfo, actInfo, ...
    ObservationInputNames = "Observations", ...
    ActionInputNames = "Actions");

critic2 = rlQValueFunction(criticNetwork2, obsInfo, actInfo, ...
    ObservationInputNames = "Observations", ...
    ActionInputNames = "Actions");

%% === Define Actor ===
actor = rlContinuousDeterministicActor(actorNetwork, obsInfo, actInfo, ...
    ObservationInputNames = "Observations");

%% === Agent Options ===
agentOpts = rlTD3AgentOptions;
agentOpts.SampleTime = Ts;

% --- Learning parameters ---
agentOpts.DiscountFactor = 0.998;         % Long-horizon reward (for stable gait)
agentOpts.MiniBatchSize = 128;
agentOpts.ExperienceBufferLength = 1e6;
agentOpts.TargetSmoothFactor = 5e-3;      % Soft target update
agentOpts.TargetUpdateFrequency = 2;      % TD3 delayed policy updates
agentOpts.MaxMiniBatchPerEpoch = 500;
agentOpts.NumWarmStartSteps = 10000;

% --- Target policy smoothing ---
agentOpts.TargetPolicySmoothModel.Variance = 0.2;
agentOpts.TargetPolicySmoothModel.LowerLimit = -0.5;
agentOpts.TargetPolicySmoothModel.UpperLimit = 0.5;

% --- Exploration noise (OU process) ---
ouNoise = rl.option.OrnsteinUhlenbeckActionNoise;
ouNoise.Mean = 0;
ouNoise.MeanAttractionConstant = 0.15;    % Slow drift â†’ smoother motion
ouNoise.Variance = 0.35;                   % Initial exploration intensity
ouNoise.VarianceDecayRate = 1e-6;         % Gradually reduce noise
ouNoise.VarianceMin = 0.1;               % Keep small exploration for fine-tuning
agentOpts.ExplorationModel = ouNoise;

% --- Assign optimizer options ---
agentOpts.ActorOptimizerOptions = actorOpts;
agentOpts.CriticOptimizerOptions = criticOpts;

%% === Build TD3 Agent ===
agent = rlTD3Agent(actor, [critic1, critic2], agentOpts);

end